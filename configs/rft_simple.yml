# Config for a simple 256 -> 16 autoencoder
model:
  model_id: rft
  sample_size: 64
  channels: 3
  
  n_layers: 12
  n_heads: 6
  d_model: 384

  patch_size: 4

train:
  trainer_id: rft
  data_id: mnist

  target_batch_size: 32
  batch_size: 32

  epochs: 200

  opt: AdamW
  opt_kwargs:
    lr: 1.0e-4
    eps: 1.0e-15
    betas: [0.9, 0.95]

  scheduler: LinearWarmup
  scheduler_kwargs:
    warmup_steps: 3000
    min_lr: 1.0e-5

  checkpoint_dir: checkpoints/v0

  sample_interval: 1000
  save_interval: 10000

wandb:
  name: shahbuland
  project: rft_test
  run_name: v0